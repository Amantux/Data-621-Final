---
title: "Final_Project"
author: "AZM"
date: "2024-11-15"
output: html_document
---



**Deliverable 1:** Project presentation slides (50 points; 5% of grade): This is where you create a concise PowerPoint presentation for a broader audience. You will submit your presentation in blackboard. You don’t need to present  in  class.  Please  upload  the  slides (in PDF format) on Brightspace by  12/15/2024 , 11:59pm EST. 

**Deliverable 2:** Final Report (250 points; 25% of grade): The final project report should be similar to the technical papers you read in the literature. The report should not exceed 12 single-spaced pages. (Appendices do not count in the page limit). Please number the pages. Please upload the report (in PDF format) on Brightspace by 12/15/2024, 11:59pm EST.**     

**The report should (at least) include the following sections:**       
**Abstract:** Use 250 words or less to summarize your problem, methodology, and major outcomes.       
**Keywords:** Select a few keywords (up to five) related to your work.        
**Introduction:** Describe the background and motivation of your problem.       
**Literature review:** Discuss how other researchers have addressed similar problems, what their
achievements are, and what the advantage and drawbacks of each reviewed approach are. Explain how your
investigation is similar or different to the state-of-the-art. Please cite the relevant papers where
appropriate.     
**Methodology:** Discuss the key aspects of your problem, data set and regression model(s). Given that you are working on real-world data, explain at a high-level your exploratory data analysis, how you prepared the data for regression modeling, your process for building regression models, and your model selection.     
**Experimentation and Results:** Describe the specifics of what you did (data exploration, data
preparation, model building, model selection, model evaluation, etc.), and what you found out
(statistical analyses, interpretation and discussion of the results, etc.).     
**Discussion and Conclusions:** Conclude your findings, and limitations, and suggest areas for future work.    
**References:** Be sure to cite all references used in the report (APA format)    
**Appendices:**    
**Supplemental tables and/or figures.**    
**R statistical programming code.**

-------------------------------------------------PAPER STARTS HERE----------------

**Abstract**      
**Keywords**      
**Introduction**     
The Russo-Ukrainian War has garnered a lot of media attention since it's beginning in 2014.  Aid in the
form of weapons sales has been a highly politicized topic in the United States and other countries in
the world.  

Stockholm International Peace Research Institute(SIPRI) Arms Transfers database tracks information
regarding orders and deliveries of major conventional weapons between two countries. SIPRI developed a
standardized unit, the Trend Indicator Value (TVI), to measure the value in terms of "scale" of each
arms transfer and serves as a proxy for the military value of an item taking into account different
indicators.  These indicators include weight, speed, payload, type of electronics, equipment age,
whether the item(s) has been used in service by another armed forces.  As such, the TIV score allows
for a standardized comparision of different weapons types in terms of miltiary value.  Each transaction
is essentially quantified in terms of mililatry value  

The purpose of this analysis is assess weapons transfers to Ukraine between the years of 2014 to 2023.
Utilizing features in the SIPRI database, the analysis seeks to uncover relationships which might
contribute to TIV. Of particular interest are the effect
of the features 'supplier' (country of weapon supplier) and 'Weapon designation' (type of weapon)

The Null hypothesis for this analysis is: 

*H0: There is no statistically significant relationship between the independent variables and SIPRI TIV total order.*

*HA: At least one of the independent variables (Recipient, Supplier, Year of Order, Number Ordered, Weapon Designation, Weapon Description, Number Delivered, Years of Delivery, or Status) has a statistically significant effect on the SIPRI TIV for total order.*

**Literature review**
**Methodology**
**Experimentation and Results**
**Discussion and Conclusions**
**References**
**Appendices**
**Supplemental tables and/or figures**
**R statistical programming code**

```{r}
library(dplyr)     #For data manipulation
library(caret)     #For machine learning models
library(tidyr)     #For data cleaning
library(ggcorrplot)
library(ggplot2)
library(dendextend)
library(reshape2)
library(polycor)
```


```{r}
# Read the CSV file
trade_register <- read.csv("https://raw.githubusercontent.com/Amantux/Data-621-Final/refs/heads/main/trade-register.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE)
# Check the first few rows to verify data import
#head(trade_register)
trade_register <- subset(trade_register, Recipient == "Ukraine")
trade_register
```

```{r}
#Convert to numeric
trade_register$SIPRI.TIV.for.total.order <- as.numeric(trade_register$SIPRI.TIV.for.total.order)

#Count introduced NAs
num_na_introduced <- sum(is.na(trade_register$SIPRI.TIV.for.total.order))

#Print warning if any
if(num_na_introduced > 0) {
  print(paste("Warning:", num_na_introduced, "NA values introduced"))
}

#Replace NAs with 0
trade_register$SIPRI.TIV.for.total.order[is.na(trade_register$SIPRI.TIV.for.total.order)] <- 0

```



```{r}
#Summary stats for all columns
summary_stats <- summary(trade_register)

#Count NAs per column
na_counts <- sapply(trade_register, function(x) sum(is.na(x)))

#Flag irregularities
irregularities <- data.frame(
  Column = names(na_counts),
  NA_Count = na_counts,
  Zero_Count = sapply(trade_register, function(x) if(is.numeric(x)) sum(x == 0, na.rm = TRUE) else NA),
  Outliers = sapply(trade_register, function(x) {
    if(is.numeric(x)) {
      q1 <- quantile(x, 0.25, na.rm = TRUE)
      q3 <- quantile(x, 0.75, na.rm = TRUE)
      iqr <- q3 - q1
      sum(x < (q1 - 1.5 * iqr) | x > (q3 + 1.5 * iqr), na.rm = TRUE)
    } else {
      NA
    }
  })
)

#Print irregularities
print("Summary Stats:")
summary_stats

print("Irregularities:")
print(irregularities)


print("Missing TIV Values")
#Filter rows with NA in SIPRI.TIV.of.delivered.weapons
na_rows <- trade_register[is.na(trade_register$SIPRI.TIV.of.delivered.weapons), ]

#View the filtered rows
print(na_rows)
```
Lets see whats going on with those weird rows with just pure ints. The are in the data. There is no explanation. I have reached out to see
Let's run a quality check to ensure that every row has a supplier and recipient

```{r}
#Create a function to flag missing or irregular values
is_missing_or_irregular <- function(x) {
  if (is.character(x)) {
    return(is.na(x) | x == "" | grepl("^\\s*$", x) | !grepl("[a-zA-Z]", x))
  } else {
    return(is.na(x))
  }
}

flagged_rows <- trade_register %>%
  filter(!grepl("[a-zA-Z]", Recipient) | !grepl("[a-zA-Z]", Supplier))

#View flagged rows
print(flagged_rows)
```

Lets remove those useless rows
```{r}
#Drop rows where Recipient or Supplier does not contain a-z
trade_register <- trade_register %>%
  filter(grepl("[a-zA-Z]", Recipient) & grepl("[a-zA-Z]", Supplier))

#Lets remove the missing flag too
fields_to_remove <- c("Missing_Flag") #Add any fields you want to remove

#Remove fields that exist in the dataset
trade_register <- trade_register %>%
  select(-all_of(intersect(fields_to_remove, names(trade_register))))

#View the cleaned dataset
head(trade_register)
```




Cut Data for Faster Processing & Testing
```{r}
set.seed(42)  # For reproducibility
sample_size <- 500  # Define sample size
#trade_register_sample <- trade_register[sample(1:nrow(trade_register), sample_size), ]
trade_register_sample <- trade_register
```



```{r}
#Identify categorical/numeric cols in the sample
categorical_cols <- names(Filter(function(x) is.factor(x) || is.character(x), trade_register_sample))
numeric_cols <- names(Filter(is.numeric, trade_register_sample))

#Init results df
anova_results <- data.frame(Categorical_Variable=character(),
                            Numeric_Variable=character(),
                            P_Value=numeric(),
                            stringsAsFactors=FALSE)

#Run ANOVA for each pair
for(cat in categorical_cols) {
  for(num in numeric_cols) {
    tryCatch({
      #Run ANOVA
      formula <- as.formula(paste(num, "~", cat))
      anova_result <- summary(aov(formula, data=trade_register_sample))
      
      #Extract p-value
      p_value <- anova_result[[1]]$`Pr(>F)`[1]
      
      
      
      #Store result
      anova_results <- rbind(anova_results, data.frame(
        Categorical_Variable=cat,
        Numeric_Variable=num,
        P_Value=p_value
      ))
    }, error=function(e) {
      #Handle error
      anova_results <- rbind(anova_results, data.frame(
        Categorical_Variable=cat,
        Numeric_Variable=num,
        P_Value=NA
      ))
    })
  }
}

#Drop NA rows
anova_results <- na.omit(anova_results)

#View results
print(anova_results)

anova_matrix <- dcast(anova_results, Numeric_Variable ~ Categorical_Variable, value.var = "P_Value")

# Check for correct column names after melting
anova_long <- melt(anova_matrix, id.vars = "Numeric_Variable", variable.name = "Categorical_Variable", value.name = "P_Value")

# Create a heatmap for the p-values
ggplot(anova_long, aes(x = Categorical_Variable, y = Numeric_Variable, fill = P_Value)) +
  geom_tile(color = "white") +
  geom_text(aes(label = ifelse(!is.na(P_Value), round(P_Value, 3), "")), 
            size = 3, color = "black") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0.05, limit = c(0, 1)
  ) +
  labs(
    title = "ANOVA-Based P-Value Heatmap",
    x = "Categorical Variable", 
    y = "Numeric Variable", 
    fill = "P-Value"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank()
  )
```


Let's go for a more advanced method: eta-squared
```{r}
data <- trade_register_sample

# Identify categorical and numeric columns
categorical_cols <- names(Filter(function(x) is.factor(x) || is.character(x), data))
numeric_cols <- names(Filter(is.numeric, data))

# Initialize results data frame
results <- data.frame(
  Variable_1 = character(),
  Variable_2 = character(),
  Eta_Squared = numeric(),
  Measure = character(),
  stringsAsFactors = FALSE
)

# Loop through all pairs of columns
for (col1 in names(data)) {
  for (col2 in names(data)) {
    if (col1 != col2) {
      tryCatch({
        # Categorical vs Numeric: Use ANOVA to compute Eta-Squared
        if (col1 %in% categorical_cols && col2 %in% numeric_cols) {
          formula <- as.formula(paste(col2, "~", col1))
          anova_model <- aov(formula, data = data)
          anova_table <- anova(anova_model)
          ss_total <- sum(anova_table$`Sum Sq`, na.rm = TRUE)
          ss_model <- anova_table$`Sum Sq`[1]
          eta_squared <- ifelse(ss_total != 0, ss_model / ss_total, NA)
          results <- rbind(results, data.frame(
            Variable_1 = col1,
            Variable_2 = col2,
            Eta_Squared = eta_squared,
            Measure = "Eta-Squared"
          ))
        }
        
        # Numeric vs Numeric: Use R-squared from Linear Regression
        if (col1 %in% numeric_cols && col2 %in% numeric_cols) {
          formula <- as.formula(paste(col2, "~", col1))
          model <- lm(formula, data = data)
          r_squared <- summary(model)$r.squared
          results <- rbind(results, data.frame(
            Variable_1 = col1,
            Variable_2 = col2,
            Eta_Squared = r_squared,
            Measure = "R-Squared"
          ))
        }
        
        # Categorical vs Categorical: Use Cramér's V
        if (col1 %in% categorical_cols && col2 %in% categorical_cols) {
          cramers_v <- cramersV(data[[col1]], data[[col2]], useNA = "no")
          results <- rbind(results, data.frame(
            Variable_1 = col1,
            Variable_2 = col2,
            Eta_Squared = cramers_v^2,  # Use square for consistency
            Measure = "Cramer's V^2"
          ))
        }
      }, error = function(e) {
        # Handle errors (e.g., missing values)
        results <- rbind(results, data.frame(
          Variable_1 = col1,
          Variable_2 = col2,
          Eta_Squared = NA,
          Measure = "Error"
        ))
      })
    }
  }
}

# View results
print(results)
```


```{r}
#Filter results to include only Eta-Squared, R-Squared, and Cramér's V^2
heatmap_data <- results %>%
  filter(Measure %in% c("Eta-Squared", "R-Squared", "Cramer's V^2")) %>%
  mutate(Eta_Squared = ifelse(is.na(Eta_Squared), 0, Eta_Squared))  # Replace NA with 0 for plotting

#Create a heatmap of effect sizes
ggplot(heatmap_data, aes(x = Variable_1, y = Variable_2, fill = Eta_Squared)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(Eta_Squared, 2)), size = 3, color = "black") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0.1, limit = c(0, 1)) +
  labs(
    title = "Effect Size Heatmap (Eta-Squared, R-Squared, Cramer's V^2)",
    x = "Variable 1",
    y = "Variable 2",
    fill = "Effect Size"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank()
  )
```
```{r}
#Filter and pivot the results to create a symmetric matrix
results_filtered <- results[results$Measure == "Eta-Squared", ]  # Filter only Eta-Squared values

#Create a full list of unique variables (to ensure symmetry)
all_vars <- unique(c(results_filtered$Variable_1, results_filtered$Variable_2))

#Create an empty symmetric matrix with all variables
empty_matrix <- matrix(0, nrow = length(all_vars), ncol = length(all_vars),
                       dimnames = list(all_vars, all_vars))

#Fill the matrix with Eta-Squared values
for (i in 1:nrow(results_filtered)) {
  var1 <- results_filtered$Variable_1[i]
  var2 <- results_filtered$Variable_2[i]
  eta_sq <- results_filtered$Eta_Squared[i]
  
  # Fill both [var1, var2] and [var2, var1] for symmetry
  empty_matrix[var1, var2] <- eta_sq
  empty_matrix[var2, var1] <- eta_sq
}

# Convert the symmetric matrix into a distance matrix
dist_matrix <- as.dist(1 - empty_matrix)  # Use (1 - Eta-Squared) as distance

# Perform hierarchical clustering
hclust_result <- hclust(dist_matrix, method = "average")  # Average linkage method

# Plot the dendrogram
plot(hclust_result, main = "Dendrogram of Variables Based on Eta-Squared",
     xlab = "", sub = "", cex = 0.8)

# Enhanced dendrogram visualization with colors
dend <- as.dendrogram(hclust_result)
dend <- color_branches(dend, k = 3)  # Color branches into 3 clusters
plot(dend, main = "Enhanced Dendrogram of Variables")
```



Let's start our model building:
```{r}
data <- na.omit(trade_register_sample)  # Or use imputation if preferred




data$Recipient <- as.factor(data$Recipient)
data$Supplier <- as.factor(data$Supplier)
data$status <- as.factor(data$status)
data$Weapon.designation <- as.factor(data$Weapon.designation)
data$Weapon.description <- as.factor(data$Weapon.description)
data$Year.of.order <- scale(data$Year.of.order)

# Define the full model with all predictors
#Remove Recipient for Urkaine
full_model <- lm(SIPRI.TIV.for.total.order ~ Supplier + Year.of.order + 
                     Number.ordered + Weapon.designation + Weapon.description + 
                     Number.delivered + Year.s..of.delivery + status, 
                 data = data)

# Define the null model (intercept-only)
null_model <- lm(SIPRI.TIV.for.total.order ~ 1, data = data)

# Perform stepwise regression based on AIC
stepwise_model <- step(null_model, 
                       scope = list(lower = null_model, upper = full_model), 
                       direction = "both", 
                       trace = TRUE)

# View the final model
summary(stepwise_model)
```





**Assumptions**
Linearity: Residuals vs Fitted Values Plot
In this plot, the residuals appear to fan out as the fitted values increase (wider spread toward the right), indicating heteroscedasticity (non-constant variance of residuals).
1. May need to log transformation or Box-Cox transformation to address non-linearity and heteroscedasticity.
2. Identify and consider removing or addressing outliers to reduce their influence.

Normality of Residuals: Histogram and Q-Q Plot
Histogram of residuals is normal, but the Q-Q plot denotes skewed tails.
This alludes to outliers in the data, possible non-normality in the dependent variable and potential heteroscedasticity (check using a scale-location plot).
1. transform the dependent variable.
```{r}
# Fit the linear model
model <- lm(SIPRI.TIV.for.total.order ~ Weapon.designation + 
              Supplier + status + Number.delivered + 
              Weapon.description + Year.of.order, data = data)

# 1. Linearity: Residuals vs Fitted Values Plot
plot(model$fitted.values, model$residuals,
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residuals vs Fitted Values")
abline(h = 0, col = "red", lty = 2)

# Add a smoother to detect patterns
library(car)  # For scatterplot smoother
scatterplot(model$fitted.values, model$residuals, smoother = TRUE, main = "Linearity Check")

# 2. Normality of Residuals: Histogram and Q-Q Plot
par(mfrow = c(1, 2))  # Set up plotting space
hist(model$residuals, breaks = 20, main = "Histogram of Residuals", xlab = "Residuals")
qqnorm(model$residuals, main = "Q-Q Plot")
qqline(model$residuals, col = "red")

# 3. Homoscedasticity: Scale-Location Plot
plot(model$fitted.values, sqrt(abs(model$residuals)),
     xlab = "Fitted Values",
     ylab = "Square Root of |Residuals|",
     main = "Scale-Location Plot")
abline(h = 0, col = "red", lty = 2)

# 4. Multicollinearity: Variance Inflation Factor (VIF)
vif_values <- vif(model)
print(vif_values)

# 5. Independence of Residuals: Durbin-Watson Test
library(lmtest)
dw_test <- dwtest(model)
print(dw_test)
```
Below the dependent variable distribution is log transformed and residuals again plotted.  
```{r}
# Load necessary libraries
library(ggplot2)
library(e1071)  # For skewness
library(moments)  # For kurtosis
#install.packages("moments")
# Plot the distribution
ggplot(data, aes(x = SIPRI.TIV.for.total.order)) +
  geom_histogram(bins = 20, fill = "blue", color = "black", alpha = 0.7) +
  geom_density(color = "red", size = 1) +
  labs(title = "Distribution of SIPRI TIV for Total Order",
       x = "SIPRI TIV for Total Order",
       y = "Frequency") +
  theme_minimal()

# Calculate skewness and kurtosis
sipri_skewness <- skewness(data$SIPRI.TIV.for.total.order, na.rm = TRUE)
sipri_kurtosis <- kurtosis(data$SIPRI.TIV.for.total.order, na.rm = TRUE)

# Print the results
cat("Skewness:", sipri_skewness, "\n")
cat("Kurtosis:", sipri_kurtosis, "\n")


data$SIPRI.TIV.for.total.order.log <- log(data$SIPRI.TIV.for.total.order + 1)

ggplot(data, aes(x = SIPRI.TIV.for.total.order.log)) +
  geom_histogram(bins = 20, fill = "blue", color = "black", alpha = 0.7) +
  geom_density(color = "red", size = 1) +
  labs(title = "Distribution of SIPRI TIV for Total OrderLog Transform",
       x = "SIPRI TIV for Total Order",
       y = "Frequency") +
  theme_minimal()

# Calculate skewness and kurtosis
sipri_skewness_log <- skewness(data$SIPRI.TIV.for.total.order.log, na.rm = TRUE)
sipri_kurtosis_log <- kurtosis(data$SIPRI.TIV.for.total.order.log, na.rm = TRUE)

# Print the results
cat("Skewness Log:", sipri_skewness_log, "\n")
cat("Kurtosis Log:", sipri_kurtosis_log, "\n")

#Original Data (Untransformed)
#Skewness: 5.85686

#A skewness value > 1 indicates a highly right-skewed distribution. This means the data has a long tail #on the right side, with extreme high values pulling the mean away from the median.
#Kurtosis: 47.41724

#A kurtosis value > 3 indicates a distribution with heavy tails and a sharp peak compared to a normal #distribution. This suggests many extreme values in the dataset, which can distort regression results.
#Log-Transformed Data
#Skewness Log: 0.5686021

#A skewness value closer to 0 indicates the distribution is much less skewed after the transformation. #A skewness value between -0.5 and 0.5 is typically considered approximately symmetric; here, the data #is moderately symmetric.
#Kurtosis Log: 2.905728

#A kurtosis value close to 3 suggests the distribution is approximately normal after the log #transformation. The extreme tails have been reduced, making the data less prone to the influence of #outliers.

```

```{r}
# Fit the linear model
model <- lm(SIPRI.TIV.for.total.order.log ~ Weapon.designation + 
              Supplier + status + Number.delivered + 
              Weapon.description + Year.of.order, data = data)

# 1. Linearity: Residuals vs Fitted Values Plot
plot(model$fitted.values, model$residuals,
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residuals vs Fitted Values")
abline(h = 0, col = "red", lty = 2)

# Add a smoother to detect patterns
library(car)  # For scatterplot smoother
scatterplot(model$fitted.values, model$residuals, smoother = TRUE, main = "Linearity Check")

# 2. Normality of Residuals: Histogram and Q-Q Plot
par(mfrow = c(1, 2))  # Set up plotting space
hist(model$residuals, breaks = 20, main = "Histogram of Residuals", xlab = "Residuals")
qqnorm(model$residuals, main = "Q-Q Plot")
qqline(model$residuals, col = "red")

# 3. Homoscedasticity: Scale-Location Plot
plot(model$fitted.values, sqrt(abs(model$residuals)),
     xlab = "Fitted Values",
     ylab = "Square Root of |Residuals|",
     main = "Scale-Location Plot")
abline(h = 0, col = "red", lty = 2)

# 4. Multicollinearity: Variance Inflation Factor (VIF)
vif_values <- vif(model)
print(vif_values)

# 5. Independence of Residuals: Durbin-Watson Test
library(lmtest)
dw_test <- dwtest(model)
print(dw_test)
```
```{r}
# Standardized residuals1

residuals_standardized <- rstandard(model)
# Identify points with standardized residuals > 3 or < -3
outliers <- which(abs(residuals_standardized) > 3)
print(outliers)

# Cook's Distance
cooks <- cooks.distance(model)
plot(cooks, main = "Cook's Distance", ylab = "Cook's Distance", xlab = "Observation")
abline(h = 4/(nrow(data) - length(coef(model))), col = "red")
```
```{r}

data_cleaned <- data[-outliers, ]
data_cleaned
```

```{r}
# Fit the linear model
model <- lm(SIPRI.TIV.for.total.order.log ~ Weapon.designation + 
              Supplier + status + Number.delivered + 
              Weapon.description + Year.of.order, data = data_cleaned)

# 1. Linearity: Residuals vs Fitted Values Plot
plot(model$fitted.values, model$residuals,
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residuals vs Fitted Values")
abline(h = 0, col = "red", lty = 2)

# Add a smoother to detect patterns
library(car)  # For scatterplot smoother
scatterplot(model$fitted.values, model$residuals, smoother = TRUE, main = "Linearity Check")

# 2. Normality of Residuals: Histogram and Q-Q Plot
par(mfrow = c(1, 2))  # Set up plotting space
hist(model$residuals, breaks = 20, main = "Histogram of Residuals", xlab = "Residuals")
qqnorm(model$residuals, main = "Q-Q Plot")
qqline(model$residuals, col = "red")

# 3. Homoscedasticity: Scale-Location Plot
plot(model$fitted.values, sqrt(abs(model$residuals)),
     xlab = "Fitted Values",
     ylab = "Square Root of |Residuals|",
     main = "Scale-Location Plot")
abline(h = 0, col = "red", lty = 2)

# 4. Multicollinearity: Variance Inflation Factor (VIF)
vif_values <- vif(model)
print(vif_values)

# 5. Independence of Residuals: Durbin-Watson Test
library(lmtest)
dw_test <- dwtest(model)
print(dw_test)
```




































```{r}
#Take a random sample from the dataset
set.seed(42)  #For reproducibility
sample_size <- 500  #Define sample size
#trade_register_sample <- trade_register[sample(1:nrow(trade_register), sample_size), ]

#Identify categorical and numeric columns
categorical_cols <- names(Filter(is.factor, trade_register))
numeric_cols <- names(Filter(is.numeric, trade_register))

#Store ANOVA results
anova_results <- data.frame(Relationship = character(),
                            P_Value = numeric(),
                            stringsAsFactors = FALSE)

#Perform ANOVA for each categorical-numeric pair
for (cat in categorical_cols) {
  for (num in numeric_cols) {
    formula <- as.formula(paste(num, "~", cat))  # Create the ANOVA formula
    tryCatch({
      anova_result <- summary(aov(formula, data = trade_register_sample))
      p_value <- anova_result[[1]]["Pr(>F)"][1]  # Extract the p-value
      anova_results <- rbind(anova_results, data.frame(Relationship = paste(num, "~", cat),
                                                       P_Value = p_value))
    }, error = function(e) {
      # Handle errors gracefully (e.g., due to missing data)
      anova_results <- rbind(anova_results, data.frame(Relationship = paste(num, "~", cat),
                                                       P_Value = NA))
    })
  }
}

#Display the ANOVA results
print("ANOVA Results:")
print(anova_results)

#Save results as a table for inspection
#write.csv(anova_results, "anova_results.csv", row.names = FALSE)


```



```{r}
trade_register <- trade_register %>%
  mutate(across(everything(), ~ ifelse(is.na(.), "Unknown", .))) # Replace NAs

#categorical variables to factors
categorical_columns <- c("Recipient", "Supplier", "Weapon.designation", 
                         "Weapon.description", "status", "Comments", "Year.s..of.delivery")
trade_register[categorical_columns] <- lapply(trade_register[categorical_columns], as.factor)

#Check for numerical columns and normalize if needed
numerical_columns <- c("Year.of.order", "Number.ordered", "Number.delivered")
head(trade_register)
```

```{r}
trade_register <- trade_register %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(trade_register$SIPRI.TIV.for.total.order, p = 0.8, list = FALSE)
train_data <- trade_register[trainIndex, ]
test_data <- trade_register[-trainIndex, ]
train_data
```

```{r}
#Train a linear regression model
#linear_model <- lm(SIPRI.TIV.for.total.order ~ Recipient + Supplier + Year.of.order  + Weapon.designation + Weapon.description + 
#                     Number.delivered + Year.s..of.delivery + status + Comments, 
#                   data = train_data)
```



```{r}
#Summarize the model
#summary(linear_model)

#train_data$Recipient <- as.factor(train_data$Recipient)
#test_data$Recipient <- as.factor(test_data$Recipient)

```

```{r}
# Find levels in test but not in train
#missing_in_train <- setdiff(levels(test_data$Recipient), levels(train_data$Recipient))

# Find levels in train but not in test
#missing_in_test <- setdiff(levels(train_data$Recipient), levels(test_data$Recipient))

# Print results
#print("Recipients in test but not in train:")
#print(missing_in_train)

#print("Recipients in train but not in test:")
#print(missing_in_test)

# Align levels between train and test
#test_data$Recipient <- factor(test_data$Recipient, levels = levels(train_data$Recipient))

# Assign unknown levels to "Unknown"
#test_data$Recipient[is.na(test_data$Recipient)] <- "Unknown"

# Predict using the trained model
#test_data <- test_data[!test_data$Recipient %in% c("106", "155", "162"), ]
#test_data <- test_data[!test_data$Supplier  %in% c("6", "10", "34", "35"), ]
#predictions <- predict(linear_model, newdata = test_data)

#Evaluate performance
#postResample(linear_predictions, test_data$SIPRI.TIV.for.total.order)
```