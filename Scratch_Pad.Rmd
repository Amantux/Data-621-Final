---
title: "Final_Project"
author: "AZM"
date: "2024-11-15"
output: html_document
---
```{r}
library(dplyr)     #For data manipulation
library(caret)     #For machine learning models
library(tidyr)     #For data cleaning
library(ggcorrplot)
library(ggplot2)
library(dendextend)
library(reshape2)
library(polycor)
library(glmnet)


```


```{r}
# Read the CSV file
trade_register <- read.csv("trade-register.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE)

# Check the first few rows to verify data import
#head(trade_register)
trade_register <- subset(trade_register, Recipient == "Ukraine")
trade_register <- trade_register %>% select(-X, -X.1, -X.2)
trade_register

```

```{r}
# Identify numeric columns in the trade_register DataFrame
numeric_columns <- sapply(trade_register, is.numeric)

# Exclude columns related to "TIV" from the numeric columns
filtered_columns <- setdiff(names(trade_register)[numeric_columns], grep("TIV", names(trade_register), value = TRUE))

# Generate scatter plots with trendlines
for (col in filtered_columns) {
  ggplot(data = trade_register, aes_string(x = col, y = "SIPRI.TIV.per.unit")) +
    geom_point(alpha = 0.7) +  # Scatter points
    geom_smooth(method = "lm", se = TRUE, color = "blue") +  # Trendline with confidence interval
    labs(
      title = paste("Comparison of", col, "with SIPRI.TIV.per.unit"),
      x = col,
      y = "SIPRI.TIV.per.unit"
    ) +
    theme_minimal() -> plot
  
  print(plot)  # Display each plot
}

  
```


```{r}
#Summary stats for all columns
summary_stats <- summary(trade_register)

#Count NAs per column
na_counts <- sapply(trade_register, function(x) sum(is.na(x)))

#Flag irregularities
irregularities <- data.frame(
  Column = names(na_counts),
  NA_Count = na_counts,
  Zero_Count = sapply(trade_register, function(x) if(is.numeric(x)) sum(x == 0, na.rm = TRUE) else NA),
  Outliers = sapply(trade_register, function(x) {
    if(is.numeric(x)) {
      q1 <- quantile(x, 0.25, na.rm = TRUE)
      q3 <- quantile(x, 0.75, na.rm = TRUE)
      iqr <- q3 - q1
      sum(x < (q1 - 1.5 * iqr) | x > (q3 + 1.5 * iqr), na.rm = TRUE)
    } else {
      NA
    }
  })
)

#Print irregularities
print("Summary Stats:")
summary_stats

print("Irregularities:")
print(irregularities)


print("Missing TIV Values")
#Filter rows with NA in SIPRI.TIV.of.delivered.weapons
na_rows <- trade_register[is.na(trade_register$SIPRI.TIV.of.delivered.weapons), ]

#View the filtered rows
print(na_rows)
```
Lets see whats going on with those weird rows with just pure ints. The are in the data. There is no explanation. I have reached out to see
Let's run a quality check to ensure that every row has a supplier and recipient

```{r}
#Create a function to flag missing or irregular values
is_missing_or_irregular <- function(x) {
  if (is.character(x)) {
    return(is.na(x) | x == "" | grepl("^\\s*$", x) | !grepl("[a-zA-Z]", x))
  } else {
    return(is.na(x))
  }
}

flagged_rows <- trade_register %>%
  filter(!grepl("[a-zA-Z]", Recipient) | !grepl("[a-zA-Z]", Supplier))

#View flagged rows
print(flagged_rows)
```

Lets remove those useless rows
```{r}
#Drop rows where Recipient or Supplier does not contain a-z
trade_register <- trade_register %>%
  filter(grepl("[a-zA-Z]", Recipient) & grepl("[a-zA-Z]", Supplier))

#Lets remove the missing flag too
fields_to_remove <- c("Missing_Flag") #Add any fields you want to remove

#Remove fields that exist in the dataset
trade_register <- trade_register %>%
  select(-all_of(intersect(fields_to_remove, names(trade_register))))

#View the cleaned dataset
head(trade_register)
```

#Cut Data for Faster Processing & Testing
Data Cut not needed as Ukrainian conflict is small enough
```{r}
#set.seed(42)  # For reproducibility
#sample_size <- 500  # Define sample size
#trade_register_sample <- trade_register[sample(1:nrow(trade_register), sample_size), ]
trade_register_sample <- trade_register
```



```{r}
#Identify categorical/numeric cols in the sample
categorical_cols <- names(Filter(function(x) is.factor(x) || is.character(x), trade_register_sample))
numeric_cols <- names(Filter(is.numeric, trade_register_sample))

#Init results df
anova_results <- data.frame(Categorical_Variable=character(),
                            Numeric_Variable=character(),
                            P_Value=numeric(),
                            stringsAsFactors=FALSE)

#Run ANOVA for each pair
for(cat in categorical_cols) {
  for(num in numeric_cols) {
    tryCatch({
      #Run ANOVA
      formula <- as.formula(paste(num, "~", cat))
      anova_result <- summary(aov(formula, data=trade_register_sample))
      
      #Extract p-value
      p_value <- anova_result[[1]]$`Pr(>F)`[1]
      
      
      
      #Store result
      anova_results <- rbind(anova_results, data.frame(
        Categorical_Variable=cat,
        Numeric_Variable=num,
        P_Value=p_value
      ))
    }, error=function(e) {
      #Handle error
      anova_results <- rbind(anova_results, data.frame(
        Categorical_Variable=cat,
        Numeric_Variable=num,
        P_Value=NA
      ))
    })
  }
}

#Drop NA rows
anova_results <- na.omit(anova_results)

#View results
print(anova_results)

anova_matrix <- dcast(anova_results, Numeric_Variable ~ Categorical_Variable, value.var = "P_Value")

# Check for correct column names after melting
anova_long <- melt(anova_matrix, id.vars = "Numeric_Variable", variable.name = "Categorical_Variable", value.name = "P_Value")

# Create a heatmap for the p-values
ggplot(anova_long, aes(x = Categorical_Variable, y = Numeric_Variable, fill = P_Value)) +
  geom_tile(color = "white") +
  geom_text(aes(label = ifelse(!is.na(P_Value), round(P_Value, 3), "")), 
            size = 3, color = "black") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0.05, limit = c(0, 1)
  ) +
  labs(
    title = "ANOVA-Based P-Value Heatmap",
    x = "Categorical Variable", 
    y = "Numeric Variable", 
    fill = "P-Value"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank()
  )
```


Let's go for a more advanced method: eta-squared
```{r}
data <- trade_register_sample

# Identify categorical and numeric columns
categorical_cols <- names(Filter(function(x) is.factor(x) || is.character(x), data))
numeric_cols <- names(Filter(is.numeric, data))

# Initialize results data frame
results <- data.frame(
  Variable_1 = character(),
  Variable_2 = character(),
  Eta_Squared = numeric(),
  Measure = character(),
  stringsAsFactors = FALSE
)

# Loop through all pairs of columns
for (col1 in names(data)) {
  for (col2 in names(data)) {
    if (col1 != col2) {
      tryCatch({
        # Categorical vs Numeric: Use ANOVA to compute Eta-Squared
        if (col1 %in% categorical_cols && col2 %in% numeric_cols) {
          formula <- as.formula(paste(col2, "~", col1))
          anova_model <- aov(formula, data = data)
          anova_table <- anova(anova_model)
          ss_total <- sum(anova_table$`Sum Sq`, na.rm = TRUE)
          ss_model <- anova_table$`Sum Sq`[1]
          eta_squared <- ifelse(ss_total != 0, ss_model / ss_total, NA)
          results <- rbind(results, data.frame(
            Variable_1 = col1,
            Variable_2 = col2,
            Eta_Squared = eta_squared,
            Measure = "Eta-Squared"
          ))
        }
        
        # Numeric vs Numeric: Use R-squared from Linear Regression
        if (col1 %in% numeric_cols && col2 %in% numeric_cols) {
          formula <- as.formula(paste(col2, "~", col1))
          model <- lm(formula, data = data)
          r_squared <- summary(model)$r.squared
          results <- rbind(results, data.frame(
            Variable_1 = col1,
            Variable_2 = col2,
            Eta_Squared = r_squared,
            Measure = "R-Squared"
          ))
        }
        
        # Categorical vs Categorical: Use Cramér's V
        if (col1 %in% categorical_cols && col2 %in% categorical_cols) {
          cramers_v <- cramersV(data[[col1]], data[[col2]], useNA = "no")
          results <- rbind(results, data.frame(
            Variable_1 = col1,
            Variable_2 = col2,
            Eta_Squared = cramers_v^2,  # Use square for consistency
            Measure = "Cramer's V^2"
          ))
        }
      }, error = function(e) {
        # Handle errors (e.g., missing values)
        results <- rbind(results, data.frame(
          Variable_1 = col1,
          Variable_2 = col2,
          Eta_Squared = NA,
          Measure = "Error"
        ))
      })
    }
  }
}

# View results
print(results)
```


```{r}
#Filter results to include only Eta-Squared, R-Squared, and Cramér's V^2
heatmap_data <- results %>%
  filter(Measure %in% c("Eta-Squared", "R-Squared", "Cramer's V^2")) %>%
  mutate(Eta_Squared = ifelse(is.na(Eta_Squared), 0, Eta_Squared))  # Replace NA with 0 for plotting

#Create a heatmap of effect sizes
ggplot(heatmap_data, aes(x = Variable_1, y = Variable_2, fill = Eta_Squared)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(Eta_Squared, 2)), size = 3, color = "black") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0.1, limit = c(0, 1)) +
  labs(
    title = "Effect Size Heatmap (Eta-Squared, R-Squared, Cramer's V^2)",
    x = "Variable 1",
    y = "Variable 2",
    fill = "Effect Size"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank()
  )
```
```{r}
#Filter and pivot the results to create a symmetric matrix
results_filtered <- results[results$Measure == "Eta-Squared", ]  # Filter only Eta-Squared values

#Create a full list of unique variables (to ensure symmetry)
all_vars <- unique(c(results_filtered$Variable_1, results_filtered$Variable_2))

#Create an empty symmetric matrix with all variables
empty_matrix <- matrix(0, nrow = length(all_vars), ncol = length(all_vars),
                       dimnames = list(all_vars, all_vars))

#Fill the matrix with Eta-Squared values
for (i in 1:nrow(results_filtered)) {
  var1 <- results_filtered$Variable_1[i]
  var2 <- results_filtered$Variable_2[i]
  eta_sq <- results_filtered$Eta_Squared[i]
  
  # Fill both [var1, var2] and [var2, var1] for symmetry
  empty_matrix[var1, var2] <- eta_sq
  empty_matrix[var2, var1] <- eta_sq
}

# Convert the symmetric matrix into a distance matrix
dist_matrix <- as.dist(1 - empty_matrix)  # Use (1 - Eta-Squared) as distance

# Perform hierarchical clustering
hclust_result <- hclust(dist_matrix, method = "average")  # Average linkage method

# Plot the dendrogram
plot(hclust_result, main = "Dendrogram of Variables Based on Eta-Squared",
     xlab = "", sub = "", cex = 0.8)

# Enhanced dendrogram visualization with colors
dend <- as.dendrogram(hclust_result)
dend <- color_branches(dend, k = 3)  # Color branches into 3 clusters
plot(dend, main = "Enhanced Dendrogram of Variables")
```



Let's start our model building:
Were going to remove weapon Weapon.designation in order to prevent overfitting
```{r}
data <- na.omit(trade_register_sample)  # Or use imputation if preferred
data <- data %>% select(-Weapon.designation)





data$Recipient <- as.factor(data$Recipient)
data$Supplier <- as.factor(data$Supplier)
data$status <- as.factor(data$status)
#data$Weapon.designation <- as.factor(data$Weapon.designation)
data$Weapon.description <- as.factor(data$Weapon.description)
data$Year.of.order <- scale(data$Year.of.order)

# Define the full model with all predictors
#Remove Recipient for Urkaine
full_model <- lm(SIPRI.TIV.per.unit ~ Supplier + Year.of.order + 
                     Number.ordered + Weapon.description + 
                     Number.delivered + Year.s..of.delivery + status, 
                 data = data)

# Define the null model (intercept-only)
null_model <- lm(SIPRI.TIV.per.unit ~ 1, data = data)

# Perform stepwise regression based on AIC
stepwise_model <- step(null_model, 
                       scope = list(lower = null_model, upper = full_model), 
                       direction = "both", 
                       trace = TRUE)

# View the final model
summary(stepwise_model)
```
Summary Insights:
Significant Positive Effects:

    Weapon.descriptionABM/SAM system (21.68; p < 2e-16 ***): Indicates a large positive effect.
    Weapon.descriptionFGA aircraft (11.08; p = 4.25e-09 ***): Strongly positive.
    Weapon.descriptionSAM system (11.77; p = 5.43e-13 ***): Strong positive impact.
    Weapon.descriptionminehunter (17.68; p = 2.81e-15 ***): Very significant positive impact.

Significant Negative Effects:

    statusSecond hand (-0.91; p = 0.00213 **): Indicates a statistically significant reduction in the outcome when the status is second-hand.

Marginally Significant Effects:

    Weapon.descriptioncoastal defence system (3.68; p = 0.08161 .): Marginally significant positive effect.
    Weapon.descriptioncombat helicopter (3.08; p = 0.09232 .): Marginally significant positive effect.

Non-Significant Effects:

    Many categories (e.g., Weapon.descriptionAEV, Weapon.descriptionanti-ship missile/ASM/SAM) show high p-values, indicating no statistically significant relationship with the dependent variable.


```{r}
#Prepare the data: Convert formula to design matrix
X <- model.matrix(SIPRI.TIV.per.unit ~ Supplier + Year.of.order + 
                     Number.ordered + Weapon.description + 
                     Number.delivered + Year.s..of.delivery + status, 
                  data = data)[, -1] #Exclude intercept
y <- data$SIPRI.TIV.per.unit

#Train-test split
set.seed(123)
train_indices <- sample(1:nrow(X), 0.8 * nrow(X))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]

#Ridge regression
ridge_model <- glmnet(X_train, y_train, alpha = 0)
cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0) #Cross-validation
best_lambda_ridge <- cv_ridge$lambda.min

#Lasso regression
lasso_model <- glmnet(X_train, y_train, alpha = 1)
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1) #Cross-validation
best_lambda_lasso <- cv_lasso$lambda.min

#Predictions
ridge_preds <- predict(ridge_model, s = best_lambda_ridge, newx = X_test)
lasso_preds <- predict(lasso_model, s = best_lambda_lasso, newx = X_test)

#Evaluate performance
ridge_rmse <- sqrt(mean((ridge_preds - y_test)^2))
lasso_rmse <- sqrt(mean((lasso_preds - y_test)^2))

#Output results
cat("Best Lambda (Ridge):", best_lambda_ridge, "\n")
cat("Ridge RMSE:", ridge_rmse, "\n")
cat("Best Lambda (Lasso):", best_lambda_lasso, "\n")
cat("Lasso RMSE:", lasso_rmse, "\n")
```
```{r}


#Ridge regression
ridge_model <- glmnet(X_train, y_train, alpha = 0)
cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0) #Cross-validation
best_lambda_ridge <- cv_ridge$lambda.min
ridge_preds <- predict(ridge_model, s = best_lambda_ridge, newx = X_test)
ridge_rmse <- sqrt(mean((ridge_preds - y_test)^2)) #Ridge RMSE

#Lasso regression
lasso_model <- glmnet(X_train, y_train, alpha = 1)
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1) #Cross-validation
best_lambda_lasso <- cv_lasso$lambda.min
lasso_preds <- predict(lasso_model, s = best_lambda_lasso, newx = X_test)
lasso_rmse <- sqrt(mean((lasso_preds - y_test)^2)) #Lasso RMSE

#Compare RMSE
cat("Ridge RMSE:", ridge_rmse, "\n")
cat("Lasso RMSE:", lasso_rmse, "\n")

#QQ plots for residuals
par(mfrow = c(1, 3)) #Set up for 3 plots

#Linear model QQ plot


#Ridge regression QQ plot
qqnorm(ridge_preds - y_test, main = "Ridge Residuals")
qqline(ridge_preds - y_test)

#Lasso regression QQ plot
qqnorm(lasso_preds - y_test, main = "Lasso Residuals")
qqline(lasso_preds - y_test)

par(mfrow = c(1, 1)) #Reset plot layout


```


```{r}
# Extract Ridge coefficients
ridge_coefs <- coef(ridge_model, s = best_lambda_ridge)
ridge_coefs_df <- as.data.frame(as.matrix(ridge_coefs))
ridge_coefs_df$Attribute <- rownames(ridge_coefs_df)
rownames(ridge_coefs_df) <- NULL
colnames(ridge_coefs_df)[1] <- "Ridge_Coefficients"

# Extract Lasso coefficients
lasso_coefs <- coef(lasso_model, s = best_lambda_lasso)
lasso_coefs_df <- as.data.frame(as.matrix(lasso_coefs))
lasso_coefs_df$Attribute <- rownames(lasso_coefs_df)
rownames(lasso_coefs_df) <- NULL
colnames(lasso_coefs_df)[1] <- "Lasso_Coefficients"

# Merge coefficients into a single DataFrame for comparison
coef_comparison <- merge(ridge_coefs_df, lasso_coefs_df, by = "Attribute", all = TRUE)

# Sort by absolute value of Ridge coefficients for better readability
coef_comparison <- coef_comparison[order(abs(coef_comparison$Ridge_Coefficients), decreasing = TRUE), ]

# Display the coefficient table
coef_comparison

```





